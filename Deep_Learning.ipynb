{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcEE4eaB8S2FjsB01PxRoh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agarwalsourabh55/Deep-learning/blob/deep_learning_dev/Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VlIkxaNqNpFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf86420-9c69-4bfa-bc0d-28c3f107f1a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([7.2105e-37, 0.0000e+00, 4.4842e-44])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "## Create the Empty Tensor\n",
        "x= torch.empty(3)\n",
        "print(x)\n",
        "x= torch.empty(2,3)\n",
        "x= torch.empty(2,2,3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating torch with random values "
      ],
      "metadata": {
        "id": "U_gHJK42Oinh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randn(2,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w_vaoJtOu_D",
        "outputId": "6fae67dc-953f-456a-ff26-6e83fe950399"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7233,  0.0378, -0.9199],\n",
              "        [ 1.4418, -1.2958, -0.3074]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones(2,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXlUmtq6O3c5",
        "outputId": "10d19825-34ca-4305-cccb-4037ef40b562"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Giving Specific Data type\n",
        "x.dtype  ## default float.32\n",
        "x= torch.ones(2,2,dtype=  torch.double)\n",
        "print(x.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNqvWFXiO5_H",
        "outputId": "199c9cd1-8326-42d6-cfa5-6a5502f20307"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= torch.tensor([2.5,0.1])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZamghMHeO9Ii",
        "outputId": "cbd7f5cc-5b6e-45b8-8661-0f754ab4f393"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.5000, 0.1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= torch.randn(2,2)\n",
        "y= torch.randn(2,2)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEULN_BaPQPA",
        "outputId": "b6f2810f-d4fb-4758-f8cd-8a46d3e2ff1e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4172,  0.2090],\n",
            "        [ 0.5973, -0.6693]])\n",
            "tensor([[ 1.6284, -1.3117],\n",
            "        [ 0.0707, -0.1412]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Addition\n",
        "z= x+y\n",
        "print(z) # Element wise addition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-r89odRPYv4",
        "outputId": "96736abd-8944-41ec-9aa3-303c98c55132"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2112, -1.1028],\n",
            "        [ 0.6680, -0.8105]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z= torch.add(x,y)\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5uerI1iPbJp",
        "outputId": "3015327b-2cc2-4459-a910-719b21dc0203"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2112, -1.1028],\n",
            "        [ 0.6680, -0.8105]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## inplace addition\n",
        "y.add_(x) ## will modify y\n",
        "\n",
        "## Every funciton that has _ aat the last will do the inplace operation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNIOv2RxPhj0",
        "outputId": "d64554c8-6688-434d-bf61-8d9a5f0ae0e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7940, -0.8938],\n",
              "        [ 1.2653, -1.4798]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## substaraction\n",
        "z= x-y\n",
        "z=torch.sub(x,y)\n"
      ],
      "metadata": {
        "id": "LaMV419kPmkl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Multipliacatino\n",
        "y.mul_(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb9e-tx5PzIr",
        "outputId": "2387cfee-3bf6-4a31-d876-c6a990f2fd06"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3312, -0.1868],\n",
              "        [ 0.7557,  0.9904]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## division \n",
        "torch.div(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks6SQbUxP2GP",
        "outputId": "0fd79724-4ddc-442f-dbdf-3bff91004f43"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.2595, -1.1188],\n",
              "        [ 0.7904, -0.6758]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## slicing Operations\n",
        "\n",
        "x= torch.rand(5,3)\n",
        "print(x[:,0])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfy3ZBxzP6iS",
        "outputId": "654b2331-20ef-4839-a5aa-21464ee15205"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3283, 0.7036, 0.6271, 0.6988, 0.4320])\n",
            "tensor([[0.3283, 0.1578, 0.4797],\n",
            "        [0.7036, 0.8818, 0.5780],\n",
            "        [0.6271, 0.4285, 0.4365],\n",
            "        [0.6988, 0.3112, 0.2248],\n",
            "        [0.4320, 0.7449, 0.2854]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[1,2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfO_xn5XQDbj",
        "outputId": "0837aa5e-b910-4ab6-9a52-48079b851384"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5780)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Willge tthe actual value\n",
        "x[1,2].item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOiQCNvDQNBj",
        "outputId": "d82b89ec-b2d3-41a0-b0c0-dbb94ae51f7f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5779715180397034"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## reshaping operation\n",
        "x= torch.rand(4,4)\n",
        "print(x)\n",
        "y= x.view(16)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSo6RkpJQUP3",
        "outputId": "05c5c9bb-2e1b-49bb-dacd-c26d58e62122"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2961, 0.9147, 0.7951, 0.4519],\n",
            "        [0.0882, 0.6399, 0.0094, 0.0505],\n",
            "        [0.4640, 0.1325, 0.9942, 0.0237],\n",
            "        [0.6641, 0.5004, 0.4337, 0.2985]])\n",
            "tensor([0.2961, 0.9147, 0.7951, 0.4519, 0.0882, 0.6399, 0.0094, 0.0505, 0.4640,\n",
            "        0.1325, 0.9942, 0.0237, 0.6641, 0.5004, 0.4337, 0.2985])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= x.view(-1,4)\n",
        "print(y.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzBKkJTAQezb",
        "outputId": "5d7c5766-92f0-41c7-fd6c-81b25152ae8d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#converting numpy to tensor\n",
        "a = torch.ones(5)\n",
        "print(a)\n",
        "b = a.numpy()\n",
        "print(type(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCApdPIlQuKc",
        "outputId": "602c1812-8fc4-4f7a-eebb-ed65f7e9ce75"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.add_(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGErVxkZQwwO",
        "outputId": "d7787413-1781-4cd2-cb2c-1b0476ba03a2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 2., 2., 2., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Also added +1 to the b also..because they point to the same memory location\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjD1V4aqR5IS",
        "outputId": "a436b26b-83ac-4612-9492-4091d041e532"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 2., 2., 2., 2.])\n",
            "[2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a= np.ones(5)\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMTL5kLQR7pJ",
        "outputId": "67b76793-348f-4b40-ecc8-febd41996a0b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b= torch.from_numpy(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eugmjo1QSEuf",
        "outputId": "4a94aa37-8695-4685-845c-eb61320494b0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a+=1"
      ],
      "metadata": {
        "id": "dBieziy7SJ5D"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This happen only when you have tensor on the GPU\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bh6X80WSNZ0",
        "outputId": "22d39ebe-9311-4599-c7b4-4640fb71227d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## checking cuda availability\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  x = torch.ones(5,device = device)\n",
        "  y = torch.ones(5)\n",
        "  y= y.to(device)\n",
        "\n",
        "  z= x+y\n",
        "  z.numpy()## will return error because numpy only handle CPU tensors not GPU\n",
        "  z = z.to(\"cpu\")\n",
        "else:\n",
        "  print(\"not available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCa2eHRASRAs",
        "outputId": "4a91bcb7-b587-4336-8731-1d3bad68ba13"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x = torch.ones(5,requires_grad = True)  ## tellthe pytroch that it will nees to calculate the gradients\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tl_XkmaSszh",
        "outputId": "69792fa7-fdb6-49aa-892d-b08aab03fa00"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Autgrad package and Calculating Gradients\n",
        "import torch\n",
        "x= torch.randn(3,requires_grad = True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R26Lr3KrTGsd",
        "outputId": "cfa6e09c-620d-4662-ea14-155f5cb1edb8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9422, 1.3274, 0.2494], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x+2 ## will create the computational graph    x                      this is forward pass\n",
        "                                                      # +   y\n",
        "                                                  # 2\n",
        "## pytorch will automatically create the function that will later use to creat the backpropogation function_name = Addbackward\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGTBGePHTk6C",
        "outputId": "5a0268e5-b41f-4336-a353-d9791f021d30"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.9422, 3.3274, 2.2494], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z= y*y*2\n",
        "print(z)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdEc9_ExUH5q",
        "outputId": "1afb4a40-7dd2-41de-c0b5-a1299da9d391"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([17.3136, 22.1426, 10.1199], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z= z.mean()\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAKsTgdLUR7r",
        "outputId": "45e11516-c284-408f-dc55-c629d3d33d5c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(16.5254, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward() ## tocalulat ethe gradient of z with respect tox\n",
        "print(x.grad)  ## x store that gradient\n",
        "## in backward it will creat the vector of jacobian matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L86WutxUUSI",
        "outputId": "556fc4a1-dcbe-4418-a931-b47e64fbaa78"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.9230, 4.4365, 2.9992])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= torch.randn(3,requires_grad= True)\n",
        "y= x+2\n",
        "z= y*y*2\n",
        "\n",
        "# z.backward()\n",
        "# print(x.grad)# this will give error need to add one line \n",
        "\n",
        "v = torch.tensor([0.1,1.0,0.001], dtype = torch.float32)\n",
        "z.backward(v)\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH71CtgCUcVG",
        "outputId": "1d34e456-f96c-49dc-93e9-1b378c1e3dc4"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.8598, 5.9732, 0.0117])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Call the requires_grad_(False)\n",
        "## x.detach() ## create new tensor that doesn't reauire gradieent\n",
        "## with torch.no_grad():\n",
        "\n",
        "x= torch.randn(3,requires_grad = True)\n",
        "print(x)\n",
        "x.requires_grad_(False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cZ_wFLGVfuh",
        "outputId": "d2e62ca1-bf41-41f3-b920-2b6ee7e558f3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.5585, -0.6175, -1.0777], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.5585, -0.6175, -1.0777])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x.detach()\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwJgaHGtWSsr",
        "outputId": "b66c3901-3fb4-4af9-a0e8-e4c656e31d82"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.5585, -0.6175, -1.0777])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= torch.randn(3,requires_grad = True)\n",
        "with torch.no_grad():\n",
        "  y = x+2\n",
        "  print(y) ## y don't have gradient function\n",
        "print(x) ## x has gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alRzt9e8WdsY",
        "outputId": "b1bb8afc-e33d-40ac-bb1d-40e15e1d0b58"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.6250, 2.2028, 0.5080])\n",
            "tensor([ 0.6250,  0.2028, -1.4920], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Whenever we call the backward function the value will be accumulated in the x"
      ],
      "metadata": {
        "id": "srDgqWnNWjsE"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4,requires_grad = True)\n",
        "for epoch in range(2):\n",
        "  model_output = (weights*3).sum()\n",
        "  model_output.backward()\n",
        "  print(weights.grad)\n",
        "  weights.grad.zero_()  # need to add this to prevent gradient to calculate on every iteration\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOLrKdW4Wk0V",
        "outputId": "35470fa2-37d4-4334-a857-ba02b4d9ff3f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4,requires_grad = True)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD([weights] , lr = 0.01)\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "lIbOgwSVYYcv"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4,requires_grad = True)\n",
        "\n",
        "## these are autograd package\n",
        "# z.backward()\n",
        "# weights.grad.zero_()"
      ],
      "metadata": {
        "id": "-Wfy6ZlEYuri"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Backpropogation\n",
        "\n",
        "## forward pass: compute Loss\n",
        "## Compute local gradients\n",
        "## Backward pass: Compute dLoss / dWeights using the Chain Rule\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0,requires_grad = True)\n",
        "\n",
        "## Forward pass and compute the loss\n",
        "y_hat = w*x\n",
        "loss = (y_hat - y)**2\n",
        "\n",
        "print(loss)\n",
        "\n",
        "# backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "### update weights\n",
        "### next forward and backwars\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4893Siicd9Lm",
        "outputId": "d367e75c-600e-4ed4-d324-c37c0df41082"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "X = np.array([1,2,3,4],dtype= np.float32)\n",
        "Y  = np.array([2,4,6,8],dtype = np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# Model Prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "## loss = MSE\n",
        "def loss(y,y_predicted):\n",
        "  return((y_predicted-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# MSE =1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x -y)\n",
        "\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 10\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  ## gradients\n",
        "  dw = gradient(X,Y,y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoch % 1 ==0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after Training : f(5) = {forward(5):.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JppsQ1WiIEw",
        "outputId": "5c9cc757-ee7b-498c-9c7f-70fbcd379508"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 2: w = 1.680, loss = 4.79999924\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 4: w = 1.949, loss = 0.12288000\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "epoch 6: w = 1.992, loss = 0.00314574\n",
            "epoch 7: w = 1.997, loss = 0.00050331\n",
            "epoch 8: w = 1.999, loss = 0.00008053\n",
            "epoch 9: w = 1.999, loss = 0.00001288\n",
            "epoch 10: w = 2.000, loss = 0.00000206\n",
            "Prediction after Training : f(5) = 9.999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "import torch\n",
        "\n",
        "X = torch.tensor([1,2,3,4],dtype= torch.float32)\n",
        "Y  =torch.tensor([2,4,6,8],dtype = torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0,dtype = torch.float32, requires_grad= True)\n",
        "\n",
        "# Model Prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "## loss = MSE\n",
        "def loss(y,y_predicted):\n",
        "  return((y_predicted-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# MSE =1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x -y)\n",
        "\n",
        "# def gradient(x,y,y_predicted):\n",
        "#   return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  ## gradients\n",
        "  # dw = gradient(X,Y,y_pred)\n",
        "  l.backward() ## calulate the gradient of loss dl/dw\n",
        "\n",
        "  # update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  ## empty zero gradient\n",
        "  w.grad.zero_()\n",
        "  \n",
        "  if epoch % 1 ==0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after Training : f(5) = {forward(5):.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdmpg6YuHWnq",
        "outputId": "addc3d31-d3b0-4941-df6b-5fea2ec0a862"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 2: w = 0.555, loss = 21.67499924\n",
            "epoch 3: w = 0.772, loss = 15.66018772\n",
            "epoch 4: w = 0.956, loss = 11.31448650\n",
            "epoch 5: w = 1.113, loss = 8.17471695\n",
            "epoch 6: w = 1.246, loss = 5.90623236\n",
            "epoch 7: w = 1.359, loss = 4.26725292\n",
            "epoch 8: w = 1.455, loss = 3.08308983\n",
            "epoch 9: w = 1.537, loss = 2.22753215\n",
            "epoch 10: w = 1.606, loss = 1.60939169\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 12: w = 1.716, loss = 0.84011245\n",
            "epoch 13: w = 1.758, loss = 0.60698116\n",
            "epoch 14: w = 1.794, loss = 0.43854395\n",
            "epoch 15: w = 1.825, loss = 0.31684780\n",
            "epoch 16: w = 1.851, loss = 0.22892261\n",
            "epoch 17: w = 1.874, loss = 0.16539653\n",
            "epoch 18: w = 1.893, loss = 0.11949898\n",
            "epoch 19: w = 1.909, loss = 0.08633806\n",
            "epoch 20: w = 1.922, loss = 0.06237914\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 22: w = 1.944, loss = 0.03256231\n",
            "epoch 23: w = 1.952, loss = 0.02352631\n",
            "epoch 24: w = 1.960, loss = 0.01699772\n",
            "epoch 25: w = 1.966, loss = 0.01228084\n",
            "epoch 26: w = 1.971, loss = 0.00887291\n",
            "epoch 27: w = 1.975, loss = 0.00641066\n",
            "epoch 28: w = 1.979, loss = 0.00463169\n",
            "epoch 29: w = 1.982, loss = 0.00334642\n",
            "epoch 30: w = 1.985, loss = 0.00241778\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 32: w = 1.989, loss = 0.00126211\n",
            "epoch 33: w = 1.991, loss = 0.00091188\n",
            "epoch 34: w = 1.992, loss = 0.00065882\n",
            "epoch 35: w = 1.993, loss = 0.00047601\n",
            "epoch 36: w = 1.994, loss = 0.00034392\n",
            "epoch 37: w = 1.995, loss = 0.00024848\n",
            "epoch 38: w = 1.996, loss = 0.00017952\n",
            "epoch 39: w = 1.996, loss = 0.00012971\n",
            "epoch 40: w = 1.997, loss = 0.00009371\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 42: w = 1.998, loss = 0.00004891\n",
            "epoch 43: w = 1.998, loss = 0.00003534\n",
            "epoch 44: w = 1.998, loss = 0.00002553\n",
            "epoch 45: w = 1.999, loss = 0.00001845\n",
            "epoch 46: w = 1.999, loss = 0.00001333\n",
            "epoch 47: w = 1.999, loss = 0.00000963\n",
            "epoch 48: w = 1.999, loss = 0.00000696\n",
            "epoch 49: w = 1.999, loss = 0.00000503\n",
            "epoch 50: w = 1.999, loss = 0.00000363\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 52: w = 2.000, loss = 0.00000190\n",
            "epoch 53: w = 2.000, loss = 0.00000137\n",
            "epoch 54: w = 2.000, loss = 0.00000099\n",
            "epoch 55: w = 2.000, loss = 0.00000071\n",
            "epoch 56: w = 2.000, loss = 0.00000052\n",
            "epoch 57: w = 2.000, loss = 0.00000037\n",
            "epoch 58: w = 2.000, loss = 0.00000027\n",
            "epoch 59: w = 2.000, loss = 0.00000019\n",
            "epoch 60: w = 2.000, loss = 0.00000014\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 62: w = 2.000, loss = 0.00000007\n",
            "epoch 63: w = 2.000, loss = 0.00000005\n",
            "epoch 64: w = 2.000, loss = 0.00000004\n",
            "epoch 65: w = 2.000, loss = 0.00000003\n",
            "epoch 66: w = 2.000, loss = 0.00000002\n",
            "epoch 67: w = 2.000, loss = 0.00000001\n",
            "epoch 68: w = 2.000, loss = 0.00000001\n",
            "epoch 69: w = 2.000, loss = 0.00000001\n",
            "epoch 70: w = 2.000, loss = 0.00000001\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 72: w = 2.000, loss = 0.00000000\n",
            "epoch 73: w = 2.000, loss = 0.00000000\n",
            "epoch 74: w = 2.000, loss = 0.00000000\n",
            "epoch 75: w = 2.000, loss = 0.00000000\n",
            "epoch 76: w = 2.000, loss = 0.00000000\n",
            "epoch 77: w = 2.000, loss = 0.00000000\n",
            "epoch 78: w = 2.000, loss = 0.00000000\n",
            "epoch 79: w = 2.000, loss = 0.00000000\n",
            "epoch 80: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 82: w = 2.000, loss = 0.00000000\n",
            "epoch 83: w = 2.000, loss = 0.00000000\n",
            "epoch 84: w = 2.000, loss = 0.00000000\n",
            "epoch 85: w = 2.000, loss = 0.00000000\n",
            "epoch 86: w = 2.000, loss = 0.00000000\n",
            "epoch 87: w = 2.000, loss = 0.00000000\n",
            "epoch 88: w = 2.000, loss = 0.00000000\n",
            "epoch 89: w = 2.000, loss = 0.00000000\n",
            "epoch 90: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "epoch 92: w = 2.000, loss = 0.00000000\n",
            "epoch 93: w = 2.000, loss = 0.00000000\n",
            "epoch 94: w = 2.000, loss = 0.00000000\n",
            "epoch 95: w = 2.000, loss = 0.00000000\n",
            "epoch 96: w = 2.000, loss = 0.00000000\n",
            "epoch 97: w = 2.000, loss = 0.00000000\n",
            "epoch 98: w = 2.000, loss = 0.00000000\n",
            "epoch 99: w = 2.000, loss = 0.00000000\n",
            "epoch 100: w = 2.000, loss = 0.00000000\n",
            "Prediction after Training : f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Remove manuall work from previous cell\n",
        "# 1) Design Model (input,output_size,forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#  - forward pass: compute predictions\n",
        "#  - backward pass: gradients\n",
        "#  - update weights\n",
        "#   iterate the couple of times for the best result\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "X = torch.tensor([ [1]\n",
        "                  ,[2]\n",
        "                  ,[3]\n",
        "                  ,[4]],dtype= torch.float32)\n",
        "Y  =torch.tensor([ [2]\n",
        "                  ,[4]\n",
        "                  ,[6]\n",
        "                  ,[8]],dtype = torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5],dtype = torch.float32)\n",
        "\n",
        "n_samples,n_features = X.shape\n",
        "print(n_samples,n_features)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "# model = nn.Linear(input_size,output_size)\n",
        "\n",
        "# OR\n",
        "## For custom Model\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self,input_dim, output_dim):\n",
        "    super(LinearRegression,self).__init__()\n",
        "    self.lin = nn.Linear(input_dim,output_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size , output_size)\n",
        "\n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = model(X)\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  ## gradients\n",
        "  # dw = gradient(X,Y,y_pred)\n",
        "  l.backward() ## calulate the gradient of loss dl/dw\n",
        "\n",
        "  # update weights\n",
        "  # with torch.no_grad():\n",
        "  #   w -= learning_rate * w.grad\n",
        "  optimizer.step()\n",
        "  ## empty zero gradient\n",
        "  # w.grad.zero_()\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  if epoch % 10 ==0:\n",
        "    [w,b]= model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after Training : f(5) = {model(X_test).item():.3f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erhAmB-tIo1k",
        "outputId": "ab7a1717-fdf6-445e-c1c0-ccf128937abe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Prediction before training: f(5) = -2.793\n",
            "epoch 1: w = -0.181, loss = 48.96280289\n",
            "epoch 11: w = 1.428, loss = 1.36606526\n",
            "epoch 21: w = 1.693, loss = 0.12884462\n",
            "epoch 31: w = 1.742, loss = 0.09139267\n",
            "epoch 41: w = 1.756, loss = 0.08529816\n",
            "epoch 51: w = 1.765, loss = 0.08031323\n",
            "epoch 61: w = 1.772, loss = 0.07563814\n",
            "epoch 71: w = 1.779, loss = 0.07123555\n",
            "epoch 81: w = 1.785, loss = 0.06708927\n",
            "epoch 91: w = 1.791, loss = 0.06318435\n",
            "Prediction after Training : f(5) = 9.582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0 Prepare Data\n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples= 100, n_features=1,noise=20,random_state = 1)\n",
        "X= torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y= torch.from_numpy(y_numpy.astype(np.float32))\n",
        "\n",
        "print(y.shape)\n",
        "y = y.view(y.shape[0],1)  ## Will reshape the tensor\n",
        "print(y.shape)\n",
        "\n",
        "n_samples,n_features = X.shape\n",
        "# 1 model\n",
        "\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size,output_size)\n",
        "\n",
        "\n",
        "# 2 loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr= learning_rate)\n",
        "\n",
        "# 3 training loop\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  #  Forward pass and loss\n",
        "  y_predicted = model(X)\n",
        "  loss = criterion(y_predicted,y)\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10==0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "#plot \n",
        "predicted = model(X).detach().numpy()\n",
        "plt.plot(X_numpy,y_numpy,'ro')\n",
        "plt.plot(X_numpy, predicted,'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "mcpmlrxHK70A",
        "outputId": "510da8f5-04ed-43a9-c980-c3c300dd805f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n",
            "torch.Size([100, 1])\n",
            "epoch: 10, loss = 4405.9434\n",
            "epoch: 20, loss = 3287.7805\n",
            "epoch: 30, loss = 2478.4160\n",
            "epoch: 40, loss = 1891.9624\n",
            "epoch: 50, loss = 1466.6166\n",
            "epoch: 60, loss = 1157.8466\n",
            "epoch: 70, loss = 933.5183\n",
            "epoch: 80, loss = 770.4158\n",
            "epoch: 90, loss = 651.7463\n",
            "epoch: 100, loss = 565.3505\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 0) prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X,y = bc.data , bc.target\n",
        "\n",
        "n_samples,n_features =X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y ,test_size = 0.2,random_state = 1234)\n",
        "\n",
        "\n",
        "# 1) model\n",
        "# 2) loss and optimizer\n",
        "# 3) training loop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNwlBGwiTIVQ",
        "outputId": "b6ec1b64-8ce0-4f35-97b5-1a21059e1273"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "569 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 0) prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X,y = bc.data , bc.target\n",
        "\n",
        "n_samples,n_features =X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y ,test_size = 0.2,random_state = 1234)\n",
        "\n",
        "\n",
        "sc =StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0],1)\n",
        "y_test = y_test.view(y_test.shape[0],1)\n",
        "\n",
        "# model\n",
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self,n_input_features):\n",
        "    super(LogisticRegression,self).__init__()\n",
        "    self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "\n",
        "# 2) loss and optimizer\n",
        "\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate )\n",
        "\n",
        "# 3) training loop\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #forward pass and loss\n",
        "    y_predicted = model(X_train)\n",
        "    loss = criterion(y_predicted,y_train)\n",
        "\n",
        "    #backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    #updates\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) %10 ==0:\n",
        "      print(f'epoch: {epoch+1}, loss= {loss.item():.4f}')\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_predicted = model(X_test)\n",
        "  y_predicted_cls = y_predicted.round() ## this would be part of the computation graph if we don't use with statement\n",
        "\n",
        "  acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "  print(f'accuracy= {acc:.4f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSYmicKoVPHp",
        "outputId": "8a96373d-d604-4ff9-a10f-c2180c5d32cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "569 30\n",
            "epoch: 10, loss= 0.5188\n",
            "epoch: 20, loss= 0.4349\n",
            "epoch: 30, loss= 0.3808\n",
            "epoch: 40, loss= 0.3429\n",
            "epoch: 50, loss= 0.3146\n",
            "epoch: 60, loss= 0.2926\n",
            "epoch: 70, loss= 0.2747\n",
            "epoch: 80, loss= 0.2598\n",
            "epoch: 90, loss= 0.2472\n",
            "epoch: 100, loss= 0.2364\n",
            "epoch: 110, loss= 0.2269\n",
            "epoch: 120, loss= 0.2185\n",
            "epoch: 130, loss= 0.2110\n",
            "epoch: 140, loss= 0.2042\n",
            "epoch: 150, loss= 0.1981\n",
            "epoch: 160, loss= 0.1926\n",
            "epoch: 170, loss= 0.1875\n",
            "epoch: 180, loss= 0.1828\n",
            "epoch: 190, loss= 0.1784\n",
            "epoch: 200, loss= 0.1744\n",
            "epoch: 210, loss= 0.1706\n",
            "epoch: 220, loss= 0.1671\n",
            "epoch: 230, loss= 0.1638\n",
            "epoch: 240, loss= 0.1608\n",
            "epoch: 250, loss= 0.1579\n",
            "epoch: 260, loss= 0.1551\n",
            "epoch: 270, loss= 0.1525\n",
            "epoch: 280, loss= 0.1501\n",
            "epoch: 290, loss= 0.1477\n",
            "epoch: 300, loss= 0.1455\n",
            "epoch: 310, loss= 0.1434\n",
            "epoch: 320, loss= 0.1414\n",
            "epoch: 330, loss= 0.1395\n",
            "epoch: 340, loss= 0.1377\n",
            "epoch: 350, loss= 0.1359\n",
            "epoch: 360, loss= 0.1343\n",
            "epoch: 370, loss= 0.1326\n",
            "epoch: 380, loss= 0.1311\n",
            "epoch: 390, loss= 0.1296\n",
            "epoch: 400, loss= 0.1282\n",
            "epoch: 410, loss= 0.1268\n",
            "epoch: 420, loss= 0.1255\n",
            "epoch: 430, loss= 0.1242\n",
            "epoch: 440, loss= 0.1230\n",
            "epoch: 450, loss= 0.1218\n",
            "epoch: 460, loss= 0.1207\n",
            "epoch: 470, loss= 0.1196\n",
            "epoch: 480, loss= 0.1185\n",
            "epoch: 490, loss= 0.1175\n",
            "epoch: 500, loss= 0.1165\n",
            "epoch: 510, loss= 0.1155\n",
            "epoch: 520, loss= 0.1145\n",
            "epoch: 530, loss= 0.1136\n",
            "epoch: 540, loss= 0.1127\n",
            "epoch: 550, loss= 0.1119\n",
            "epoch: 560, loss= 0.1110\n",
            "epoch: 570, loss= 0.1102\n",
            "epoch: 580, loss= 0.1094\n",
            "epoch: 590, loss= 0.1086\n",
            "epoch: 600, loss= 0.1079\n",
            "epoch: 610, loss= 0.1071\n",
            "epoch: 620, loss= 0.1064\n",
            "epoch: 630, loss= 0.1057\n",
            "epoch: 640, loss= 0.1050\n",
            "epoch: 650, loss= 0.1044\n",
            "epoch: 660, loss= 0.1037\n",
            "epoch: 670, loss= 0.1031\n",
            "epoch: 680, loss= 0.1025\n",
            "epoch: 690, loss= 0.1019\n",
            "epoch: 700, loss= 0.1013\n",
            "epoch: 710, loss= 0.1007\n",
            "epoch: 720, loss= 0.1001\n",
            "epoch: 730, loss= 0.0996\n",
            "epoch: 740, loss= 0.0990\n",
            "epoch: 750, loss= 0.0985\n",
            "epoch: 760, loss= 0.0980\n",
            "epoch: 770, loss= 0.0975\n",
            "epoch: 780, loss= 0.0970\n",
            "epoch: 790, loss= 0.0965\n",
            "epoch: 800, loss= 0.0960\n",
            "epoch: 810, loss= 0.0956\n",
            "epoch: 820, loss= 0.0951\n",
            "epoch: 830, loss= 0.0946\n",
            "epoch: 840, loss= 0.0942\n",
            "epoch: 850, loss= 0.0938\n",
            "epoch: 860, loss= 0.0933\n",
            "epoch: 870, loss= 0.0929\n",
            "epoch: 880, loss= 0.0925\n",
            "epoch: 890, loss= 0.0921\n",
            "epoch: 900, loss= 0.0917\n",
            "epoch: 910, loss= 0.0913\n",
            "epoch: 920, loss= 0.0909\n",
            "epoch: 930, loss= 0.0906\n",
            "epoch: 940, loss= 0.0902\n",
            "epoch: 950, loss= 0.0898\n",
            "epoch: 960, loss= 0.0895\n",
            "epoch: 970, loss= 0.0891\n",
            "epoch: 980, loss= 0.0888\n",
            "epoch: 990, loss= 0.0884\n",
            "epoch: 1000, loss= 0.0881\n",
            "accuracy= 0.9474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##  Dataset and Dataloader to laod wine.csv\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "epoch = 1 forward and backward pass of all training samples\n",
        "\n",
        "batch_size = number of training samples in one forward and backward pass\n",
        "number of iterations = number of passes, each pass using [batch_size] number of samples\n",
        "e.g. 100 samples , batch_size = 20--> 100/20 = 5 iterations for 1 epoch\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    # data loading\n",
        "    xy = xy = np.loadtxt('/content/wine.csv',delimiter=\",\",dtype = np.float32, skiprows =1)\n",
        "    self.x = torch.from_numpy(xy[:, 1:])\n",
        "    self.y = torch.from_numpy(xy[:,[0]]) # n_samples ,1\n",
        "    self.n_samples = xy.shape[0]\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "     # dataset\n",
        "     return self.x[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "dataset = WineDataset()\n",
        "first_data = dataset[0]\n",
        "features,labels = first_data\n",
        "print(features,labels)\n",
        "\n",
        "dataloader = DataLoader(dataset = dataset,\n",
        "                        batch_size = 4,\n",
        "                        shuffle = True,\n",
        "                        num_workers = 2)\n",
        "\n",
        "dataiterator = iter(dataloader)\n",
        "\n",
        "data = dataiterator.next()\n",
        "features,labels = data\n",
        "print(features,labels)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHysajxrXaSn",
        "outputId": "9b21f389-bad1-4cc6-8da5-cdee231b98f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03]) tensor([1.])\n",
            "tensor([[1.4380e+01, 3.5900e+00, 2.2800e+00, 1.6000e+01, 1.0200e+02, 3.2500e+00,\n",
            "         3.1700e+00, 2.7000e-01, 2.1900e+00, 4.9000e+00, 1.0400e+00, 3.4400e+00,\n",
            "         1.0650e+03],\n",
            "        [1.2080e+01, 2.0800e+00, 1.7000e+00, 1.7500e+01, 9.7000e+01, 2.2300e+00,\n",
            "         2.1700e+00, 2.6000e-01, 1.4000e+00, 3.3000e+00, 1.2700e+00, 2.9600e+00,\n",
            "         7.1000e+02],\n",
            "        [1.4130e+01, 4.1000e+00, 2.7400e+00, 2.4500e+01, 9.6000e+01, 2.0500e+00,\n",
            "         7.6000e-01, 5.6000e-01, 1.3500e+00, 9.2000e+00, 6.1000e-01, 1.6000e+00,\n",
            "         5.6000e+02],\n",
            "        [1.3510e+01, 1.8000e+00, 2.6500e+00, 1.9000e+01, 1.1000e+02, 2.3500e+00,\n",
            "         2.5300e+00, 2.9000e-01, 1.5400e+00, 4.2000e+00, 1.1000e+00, 2.8700e+00,\n",
            "         1.0950e+03]]) tensor([[1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## training loop\n",
        "## lets specify some hyperparameters\n",
        "\n",
        "num_epochs = 2\n",
        "total_samples = len(dataset)\n",
        "n_iterations = math.ceil(total_samples/4)\n",
        "print(total_samples,n_iterations)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i,(inputs,labels) in enumerate(dataloader):\n",
        "    # forward backward, update\n",
        "    if (i+1)%5 ==0:\n",
        "      print(f'epoch {epoch+1}/{num_epochs},step {i+1}/{n_iterations},inputs {inputs.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMsSqMBWlF19",
        "outputId": "1b55e42c-8c71-412c-886f-397ab90c8d54"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "178 45\n",
            "epoch 1/2,step 5/45,inputs torch.Size([4, 13])\n",
            "epoch 1/2,step 10/45,inputs torch.Size([4, 13])\n",
            "epoch 1/2,step 15/45,inputs torch.Size([4, 13])\n",
            "epoch 1/2,step 20/45,inputs torch.Size([4, 13])\n",
            "epoch 1/2,step 25/45,inputs torch.Size([4, 13])\n",
            "epoch 1/2,step 30/45,inputs torch.Size([4, 13])\n",
            "epoch 1/2,step 35/45,inputs torch.Size([4, 13])\n",
            "epoch 1/2,step 40/45,inputs torch.Size([4, 13])\n",
            "epoch 1/2,step 45/45,inputs torch.Size([2, 13])\n",
            "epoch 2/2,step 5/45,inputs torch.Size([4, 13])\n",
            "epoch 2/2,step 10/45,inputs torch.Size([4, 13])\n",
            "epoch 2/2,step 15/45,inputs torch.Size([4, 13])\n",
            "epoch 2/2,step 20/45,inputs torch.Size([4, 13])\n",
            "epoch 2/2,step 25/45,inputs torch.Size([4, 13])\n",
            "epoch 2/2,step 30/45,inputs torch.Size([4, 13])\n",
            "epoch 2/2,step 35/45,inputs torch.Size([4, 13])\n",
            "epoch 2/2,step 40/45,inputs torch.Size([4, 13])\n",
            "epoch 2/2,step 45/45,inputs torch.Size([2, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Dataset Transform\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "  def __init__(self,transform= None):\n",
        "    # data loading\n",
        "    xy = np.loadtxt('/content/wine.csv',delimiter=\",\",dtype = np.float32, skiprows =1)\n",
        "\n",
        "    self.x = xy[:, 1:]\n",
        "    self.y = xy[:,[0]] # n_samples ,1\n",
        "    self.n_samples = xy.shape[0]\n",
        "\n",
        "    self.transform = transform\n",
        "\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "     # dataset\n",
        "     sample = self.x[index], self.y[index]\n",
        "     if self.transform:\n",
        "       sample = self.transform(sample)\n",
        "\n",
        "     return sample\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "class ToTensor:\n",
        "  def __call__(self,sample):\n",
        "    inputs, targets = sample\n",
        "    return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n",
        "class MulTransform:\n",
        "  def __init__(self,factor):\n",
        "    self.factor = factor\n",
        "\n",
        "  def __call__(self,sample):\n",
        "    inputs,target = sample\n",
        "    inputs*= self.factor\n",
        "    return inputs,target\n",
        "\n",
        "\n",
        "dataset = WineDataset(transform = ToTensor())\n",
        "# dataset = WineDataset(transform = None)\n",
        "first_data = dataset[0]\n",
        "features,labels = first_data\n",
        "print(features)\n",
        "print(type(features),type(labels))\n",
        "\n",
        "\n",
        "composed = torchvision.transforms.Compose([ToTensor(),MulTransform(2)])\n",
        "dataset = WineDataset(transform = composed)\n",
        "first_data = dataset[0]\n",
        "features,labels = first_data\n",
        "print(features)\n",
        "print(type(features),type(labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a74BclbmpP6",
        "outputId": "0a96f42d-9a33-444c-deda-81d704d501f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03])\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "tensor([2.8460e+01, 3.4200e+00, 4.8600e+00, 3.1200e+01, 2.5400e+02, 5.6000e+00,\n",
            "        6.1200e+00, 5.6000e-01, 4.5800e+00, 1.1280e+01, 2.0800e+00, 7.8400e+00,\n",
            "        2.1300e+03])\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Softmax and Cross Entropy \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x),axis=0)\n",
        "\n",
        "x= np.array([2.0,1.0,0.1])\n",
        "outputs = softmax(x)\n",
        "print('softmax numpy:',outputs)\n",
        "\n",
        "x = torch.tensor([2.0,1.0,0.1])\n",
        "outputs = torch.softmax(x,dim=0)\n",
        "print(outputs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Isu0jpfoy0R",
        "outputId": "00b78476-e95b-43a6-c076-9b0696c28bd8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
            "tensor([0.6590, 0.2424, 0.0986])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def cross_entropy(actual,predicted):\n",
        "  loss = np.sum(actual * np.log(predicted))\n",
        "\n",
        "  return loss\n",
        "Y= np.array([1,0,0])\n",
        "\n",
        "# y_pred has probabilities\n",
        "Y_pred_good = np.array([0.7,0.2,0.1])\n",
        "Y_pred_bad = np.array([0.1,0.3,0.6])\n",
        "\n",
        "l1 = cross_entropy(Y,Y_pred_good)\n",
        "l2 = cross_entropy(Y,Y_pred_bad)\n",
        "\n",
        "print(f'Loss1 numpy: {l1:.4f}')\n",
        "print(f'Loss2 numpy: {l2:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFpscOiwrz9B",
        "outputId": "618f48a2-c09e-4173-be58-7c01e878a94b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss1 numpy: -0.3567\n",
            "Loss2 numpy: -2.3026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "## Carefull\n",
        "## nn.crossEntropyLoss applies\n",
        "\n",
        "## nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss)\n",
        "# -> No softmax for last\n",
        "## y havs calss labels, not onehot\n",
        "# Ypred has raw scores, not softmax\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# 1 samples\n",
        "Y = torch.tensor([0])\n",
        "\n",
        "#nsamples * nclasses =1*3\n",
        "Y_pred_good = torch.tensor([[2.0,1.0,0.1]]) ## these are raw applies didn;t applies softmax\n",
        "Y_pred_bad = torch.tensor([[0.5,2.0,0.3]])\n",
        "\n",
        "l1 = loss(Y_pred_good,Y)\n",
        "l2 = loss(Y_pred_bad,Y)\n",
        "\n",
        "print(l1.item()) ## it has lower cross entrpiy loss\n",
        "print(l2.item())\n",
        "\n",
        "_,predictions1 = torch.max(Y_pred_good,1)\n",
        "_,predictions2 = torch.max(Y_pred_bad,1)\n",
        "\n",
        "print(predictions1)\n",
        "print(predictions2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyz396jAs9u6",
        "outputId": "9b931e8b-8025-44c2-d050-b6adae5d1d78"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4170299470424652\n",
            "1.840616226196289\n",
            "tensor([0])\n",
            "tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "## Carefull\n",
        "## nn.crossEntropyLoss applies\n",
        "\n",
        "## nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss)\n",
        "# -> No softmax for last\n",
        "## y havs calss labels, not onehot\n",
        "# Ypred has raw scores, not softmax\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3 samples\n",
        "Y = torch.tensor([2,0,1])\n",
        "\n",
        "#nsamples * nclasses =1*3\n",
        "Y_pred_good = torch.tensor([[0.1,1.0,2.1],\n",
        "                            [2.0,1.0,0.1],\n",
        "                            [0.1,3.0,0.1]]) ## these are raw applies didn;t applies softmax\n",
        "\n",
        "Y_pred_bad = torch.tensor([[2.1,1.0,0.1],\n",
        "                           [0.1,1.0,2.1],\n",
        "                           [0.1,3.0,0.1]])\n",
        "\n",
        "l1 = loss(Y_pred_good,Y)\n",
        "l2 = loss(Y_pred_bad,Y)\n",
        "\n",
        "print(l1.item()) ## it has lower cross entrpiy loss\n",
        "print(l2.item())\n",
        "\n",
        "_,predictions1 = torch.max(Y_pred_good,1)\n",
        "_,predictions2 = torch.max(Y_pred_bad,1)\n",
        "\n",
        "print(predictions1)\n",
        "print(predictions2)\n",
        "\n",
        "## in Pytorch: Use nn.CrossEntropyLoss()\n",
        "# No Softmax at the end!\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8UE18f1t9mi",
        "outputId": "5f3f9b35-8e08-4909-ebfc-5bb3b957e1f2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3018244206905365\n",
            "1.6241613626480103\n",
            "tensor([2, 0, 1])\n",
            "tensor([0, 2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "## Multicalssproblem\n",
        "\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self,input_size, hidden_size,num_classes):\n",
        "    super(NeuralNet2,self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,\n",
        "                             hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    return out\n",
        "\n",
        "model = NeuralNet2(input_size = 28*28 , \n",
        "                   hidden_size = 5,\n",
        "                   num_classes = 3)\n",
        "criterion = nn.CrossEntropyLoss() ### it applies Softmax\n",
        "\n",
        "## inpytorch: use nn.BCELoss()\n",
        "# Sigmoid at the end\n"
      ],
      "metadata": {
        "id": "ALcNDu7eu6wO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "## Binary classificaiton\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self,input_size, hidden_size,num_classes):\n",
        "    super(NeuralNet2,self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,\n",
        "                             hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "\n",
        "    # Sigmoid at the end\n",
        "    y_pred = torch.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "model = NeuralNet2(input_size = 28*28 , \n",
        "                   hidden_size = 5,\n",
        "                   num_classes = 3)\n",
        "criterion = nn.BCELoss() ### it applies Softmax\n",
        "\n",
        "## inpytorch: use nn.BCELoss()\n",
        "# Sigmoid at the end\n"
      ],
      "metadata": {
        "id": "p3MuzX9lwkXd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lN21UG-cw5dE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}